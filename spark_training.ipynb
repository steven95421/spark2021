{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "import pyspark.ml as ml\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from IPython.display import display\n",
    "import glob\n",
    "from pyspark import storagelevel\n",
    "base_dir=\"/data1/r08922010/recsys2021\"\n",
    "\n",
    "files_nums=1\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/dhome/steven95421/miniconda3/envs/rapids-0.18/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/dhome/steven95421/miniconda3/envs/rapids-0.18/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_amount=3\n",
    "# concurrentGpuTasks=10\n",
    "# spark = SparkSession.builder\\\n",
    "# .master(\"spark://hpcuda2:7077\")\\\n",
    "# .config(\"spark.kryoserializer.buffer.max\", \"2047m\")\\\n",
    "# .config(\"spark.sql.files.maxPartitionBytes\", \"2047m\")\\\n",
    "# .config(\"spark.local.dir\", \"/data1/tmp_spark\")\\\n",
    "# .config(\"dfs.blocksize\", \"2047m\")\\\n",
    "# .config(\"spark.executor.cores\", 25)\\\n",
    "# .config(\"spark.executor.memory\",\"50g\")\\\n",
    "# .config(\"spark.driver.extraClassPath\",\"/opt/sparkRapidsPlugin/cudf-0.18.1-cuda11.jar:/opt/sparkRapidsPlugin/rapids-4-spark_2.12-0.4.0.jar:/opt/sparkRapidsPlugin/xgboost4j_3.0-1.3.0-0.1.0.jar:/opt/sparkRapidsPlugin/xgboost4j-spark_3.0-1.3.0-0.1.0.jar\")\\\n",
    "# .config(\"spark.executor.extraClassPath\",\"/opt/sparkRapidsPlugin/cudf-0.18.1-cuda11.jar:/opt/sparkRapidsPlugin/rapids-4-spark_2.12-0.4.0.jar:/opt/sparkRapidsPlugin/xgboost4j_3.0-1.3.0-0.1.0.jar:/opt/sparkRapidsPlugin/xgboost4j-spark_3.0-1.3.0-0.1.0.jar\")\\\n",
    "# .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\\\n",
    "# .config(\"spark.worker.resource.gpu.amount\", gpu_amount)\\\n",
    "# .config(\"spark.executor.resource.gpu.amount\", 1)\\\n",
    "# .config(\"spark.rapids.sql.concurrentGpuTasks\",concurrentGpuTasks)\\\n",
    "# .config(\"spark.task.resource.gpu.amount\", 1/concurrentGpuTasks)\\\n",
    "# .config(\"spark.rapids.memory.pinnedPool.size\", \"50G\")\\\n",
    "# .config(\"spark.rapids.memory.gpu.maxAllocFraction\", 0.5)\\\n",
    "# .config(\"spark.rapids.memory.gpu.allocFraction\", 0.5)\\\n",
    "# .config(\"spark.locality.wait\", \"0s\")\\\n",
    "# .config(\"spark.worker.resource.gpu.discoveryScript\",\"/opt/sparkRapidsPlugin/getGpusResources.sh\" )\\\n",
    "# .config(\"spark.sql.shuffle.partitions\",files_nums)\\\n",
    "# .config(\"spark.sql.files.maxPartitionBytes\",'512m')\\\n",
    "# .config(\"spark.rapids.sql.incompatibleOps.enabled\",True)\\\n",
    "# .getOrCreate()\n",
    "# spark.sparkContext.addPyFile(\"/opt/sparkRapidsPlugin/xgboost4j-spark_3.0-1.3.0-0.1.0.jar\")\n",
    "# from sparkxgb import XGBoostClassifier, XGBoostClassificationModel\n",
    "\n",
    "# print(spark.sparkContext.uiWebUrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://Milgram.csie.ntu.edu.tw:4040\n"
     ]
    }
   ],
   "source": [
    "gpu_amount=3\n",
    "concurrentGpuTasks=10\n",
    "spark = SparkSession.builder\\\n",
    ".master(\"spark://hpcuda2:7077\")\\\n",
    ".config(\"spark.kryoserializer.buffer.max\", \"2047m\")\\\n",
    ".config(\"spark.sql.files.maxPartitionBytes\", \"2047m\")\\\n",
    ".config(\"spark.local.dir\", \"/data1/tmp_spark\")\\\n",
    ".config(\"dfs.blocksize\", \"2047m\")\\\n",
    ".config(\"spark.executor.cores\", 80)\\\n",
    ".config(\"spark.executor.memory\",\"300g\")\\\n",
    ".config(\"spark.driver.memory\",\"100g\")\\\n",
    ".getOrCreate()\n",
    "spark.sparkContext.addPyFile(\"/opt/sparkRapidsPlugin/xgboost4j-spark_3.0-1.3.0-0.1.0.jar\")\n",
    "from sparkxgb import XGBoostClassifier, XGBoostClassificationModel\n",
    "\n",
    "print(spark.sparkContext.uiWebUrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "class UserTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.user_map=None\n",
    "    def _reset(self):\n",
    "        self.user_map=None\n",
    "    def fit(self,df):\n",
    "        self._reset()\n",
    "        tmp_df=df.select(\"a_user_id\").union(df.select(\"b_user_id\")).withColumnRenamed(\"a_user_id\",\"user_id_ori\")\n",
    "        self.user_map=tmp_df.distinct().withColumn(\"user_idx\", F.monotonically_increasing_id())\n",
    "    def transform(self,df):\n",
    "        df=df.join(self.user_map,df[\"a_user_id\"]==self.user_map[\"user_id_ori\"],how=\"left\").drop(\"user_id_ori\").drop(\"a_user_id\").withColumnRenamed(\"user_idx\",\"a_user_id\")\n",
    "        df=df.join(self.user_map,df[\"b_user_id\"]==self.user_map[\"user_id_ori\"],how=\"left\").drop(\"user_id_ori\").drop(\"b_user_id\").withColumnRenamed(\"user_idx\",\"b_user_id\")\n",
    "        return df\n",
    "    \n",
    "class PreprocessTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a_tweet_id_count=None\n",
    "        self.b_tweet_id_count=None\n",
    "        self.a_b_df=None\n",
    "        self.Vectorizers={}\n",
    "        self.StringIndexer=None\n",
    "        self.user_map=None\n",
    "    def _reset(self):\n",
    "        self.a_tweet_id_count=None\n",
    "        self.b_tweet_id_count=None\n",
    "        self.a_b_df=None\n",
    "        self.Vectorizers={}\n",
    "        self.StringIndexer=None\n",
    "        self.user_map=None\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        # Reset internal state before fitting\n",
    "        self._reset()\n",
    "        \n",
    "        arraytimestampCols=[\"reply\",\"retweet\",\"retweet_comment\",\"like\"]\n",
    "        array_from_unixtime = F.udf(lambda arr: 1 if arr!= None else 0,\n",
    "                           IntegerType())\n",
    "        for timestampCol in arraytimestampCols:\n",
    "            df=df.withColumn(timestampCol,\n",
    "                array_from_unixtime(df[timestampCol]))\n",
    "        \n",
    "        \n",
    "        self.a_tweet_id_count=df.groupby('a_user_id').agg(F.count(\"tweet_id\").alias(\"a_tweet_id_count\"))\n",
    "        df=df.join(self.a_tweet_id_count,on=\"a_user_id\",how=\"left\")\n",
    "        \n",
    "        self.b_tweet_id_count=df.groupby('b_user_id').agg(F.count(\"tweet_id\").alias(\"b_tweet_id_count\"))\n",
    "        df=df.join(self.b_tweet_id_count,on=\"b_user_id\",how=\"left\")\n",
    "\n",
    "        self.a_b_df=df.groupby(\"a_user_id\",'b_user_id').agg(F.count(F.lit(1)).alias(\"a_b_count\"))\n",
    "        df=df.join(self.a_b_df,on=[\"a_user_id\",'b_user_id'])\n",
    "        \n",
    "        print(df.select(*target).collect()[0].asDict())\n",
    "        \n",
    "        counting_udf = F.udf(lambda x: str(x) if x else 0,\n",
    "                           IntegerType())\n",
    "        split_string = F.udf(lambda v:v.split(\"\\t\") if v else [],\n",
    "                           ArrayType(StringType()))\n",
    "        vector_indices = F.udf(lambda v:v.indices.tolist(),\n",
    "                           ArrayType(IntegerType()))\n",
    "        hashCols=[\"hashtags\",\"links\",\"domains\"]\n",
    "        for hashCol in hashCols:\n",
    "            df = df.withColumn(hashCol, split_string(hashCol))\n",
    "            df = df.withColumn(hashCol+\"_len\", F.size(hashCol))\n",
    "            self.Vectorizers[hashCol] = CountVectorizer(minDF =10,vocabSize=1000000,inputCol=hashCol, outputCol=hashCol+\"_tokens\")\n",
    "            self.Vectorizers[hashCol]=self.Vectorizers[hashCol].fit(df)\n",
    "            print(len(self.Vectorizers[hashCol].vocabulary))\n",
    "            df = self.Vectorizers[hashCol].transform(df)\n",
    "            df = df.withColumn(hashCol,vector_indices(hashCol+\"_tokens\"))\n",
    "            df = df.drop(hashCol+\"_tokens\")\n",
    "\n",
    "        #hashing\n",
    "        labelize_inputCols=[\"media\",\"tweet_type\",\"language\"]\n",
    "        labelize_outputCols=[inputCol+\"_transformed\" for inputCol in labelize_inputCols]\n",
    "        self.StringIndexer = StringIndexer(inputCols=labelize_inputCols, outputCols=labelize_outputCols,handleInvalid=\"keep\")\n",
    "        self.StringIndexer = self.StringIndexer.fit(df)\n",
    "        df =self.StringIndexer.transform(df)\n",
    "        for labelize_inputCol in labelize_inputCols:\n",
    "            df = df.drop(labelize_inputCol).withColumnRenamed(labelize_inputCol+\"_transformed\", labelize_inputCol)\n",
    "        return df\n",
    "    def transform(self, df, update=True):\n",
    "        arraytimestampCols=[\"reply\",\"retweet\",\"retweet_comment\",\"like\"]\n",
    "        array_from_unixtime = F.udf(lambda arr: 1 if arr!= None else 0,\n",
    "                           IntegerType())\n",
    "        for timestampCol in arraytimestampCols:\n",
    "            df=df.withColumn(timestampCol,\n",
    "                array_from_unixtime(df[timestampCol]))\n",
    "            \n",
    "        if update:\n",
    "            update_tweet_id_count=df.groupby('a_user_id').agg(F.count(\"tweet_id\").alias(\"a_tweet_id_count_update\"))\n",
    "            self.a_tweet_id_count=self.a_tweet_id_count.join(update_tweet_id_count,on=\"a_user_id\",how=\"outer\").withColumn(\"a_tweet_id_count\",F.col(\"a_tweet_id_count\")+F.col(\"a_tweet_id_count_update\")).drop(\"a_tweet_id_count_update\")\n",
    "        df=df.join(self.a_tweet_id_count,on=\"a_user_id\",how=\"left\")\n",
    "        \n",
    "        if update:\n",
    "            update_tweet_id_count=df.groupby('b_user_id').agg(F.count(\"tweet_id\").alias(\"b_tweet_id_count_update\"))\n",
    "            self.b_tweet_id_count=self.b_tweet_id_count.join(update_tweet_id_count,on=\"b_user_id\",how=\"outer\").withColumn(\"b_tweet_id_count\",F.col(\"b_tweet_id_count\")+F.col(\"b_tweet_id_count_update\")).drop(\"b_tweet_id_count_update\")\n",
    "        df=df.join(self.b_tweet_id_count,on=\"b_user_id\",how=\"left\")\n",
    "\n",
    "        if update:\n",
    "            update_a_b_df=df.groupby(\"a_user_id\",'b_user_id').agg(F.count(F.lit(1)).alias(\"a_b_count_update\"))\n",
    "            self.a_b_df=self.a_b_df.join(update_a_b_df,on=[\"a_user_id\",'b_user_id'],how=\"outer\").withColumn(\"a_b_count\",F.col(\"a_b_count\")+F.col(\"a_b_count_update\")).drop(\"a_b_count_update\")\n",
    "        df=df.join(self.a_b_df,on=[\"a_user_id\",'b_user_id'])\n",
    "        \n",
    "        print(df.select(*target).collect()[0].asDict())\n",
    "        hashCols=[\"hashtags\",\"links\",\"domains\"]\n",
    "        counting_udf = F.udf(lambda x: str(x) if x else 0,\n",
    "                           IntegerType())\n",
    "        split_string = F.udf(lambda v:v.split(\"\\t\") if v else [],\n",
    "                           ArrayType(StringType()))\n",
    "        vector_indices = F.udf(lambda v:v.indices.tolist(),\n",
    "                           ArrayType(IntegerType()))\n",
    "        for hashCol in hashCols:\n",
    "            df = df.withColumn(hashCol, split_string(hashCol))\n",
    "            df = df.withColumn(hashCol+\"_len\", F.size(hashCol))\n",
    "            print(len(self.Vectorizers[hashCol].vocabulary))\n",
    "            df = self.Vectorizers[hashCol].transform(df)\n",
    "            df = df.withColumn(hashCol,vector_indices(hashCol+\"_tokens\"))\n",
    "            df = df.drop(hashCol+\"_tokens\")\n",
    "\n",
    "        #hashing\n",
    "        labelize_inputCols=[\"media\",\"tweet_type\",\"language\"]\n",
    "        labelize_outputCols=[inputCol+\"_transformed\" for inputCol in labelize_inputCols]\n",
    "        df =self.StringIndexer.transform(df)\n",
    "        for labelize_inputCol in labelize_inputCols:\n",
    "            df = df.drop(labelize_inputCol).withColumnRenamed(labelize_inputCol+\"_transformed\", labelize_inputCol)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filenames = [\n",
    "    f'{base_dir}/training/pivoted_tsv/part-00{i:03d}'.format(i=i)\n",
    "    for i in range(files_nums)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "valid_filenames = [\n",
    "    f'{base_dir}/training/pivoted_tsv/part-00{i:03d}'.format(i=i)\n",
    "    for i in range(files_nums,files_nums+files_nums)\n",
    "]\n",
    "\n",
    "# file_list=glob.glob(f'{base_dir}/training/pivoted_parquet/*parquet')\n",
    "# filenames = file_list[:50]\n",
    "\n",
    "\n",
    "\n",
    "# valid_filenames = file_list[50:50+50]\n",
    "\n",
    "\n",
    "# filenames = [\n",
    "#     '/tmp2/recsys2021/training.tsv'\n",
    "# ]\n",
    "columns = [\n",
    "    # features\n",
    "    'text_tokens', 'hashtags', 'tweet_id',\n",
    "    'media', 'links', 'domains', 'tweet_type', 'language', 'timestamp',\n",
    "    'a_user_id', 'a_follower_count', 'a_following_count', 'a_is_verified', 'a_account_creation',\n",
    "    'b_user_id', 'b_follower_count', 'b_following_count', 'b_is_verified', 'b_account_creation',\n",
    "    'a_follows_b',\n",
    "    # labels\n",
    "    'reply', 'retweet', 'retweet_comment', 'like',\n",
    "]\n",
    "target_col=['reply', 'retweet', 'retweet_comment', 'like']\n",
    "target=[F.mean(label).alias(\"mean_\"+label) for label in target_col]\n",
    "target_count=[F.sum(label).alias(\"count_\"+label) for label in target_col]\n",
    "schema = StructType([\n",
    "    StructField(\"text_tokens\", StringType(), True),\n",
    "    StructField(\"hashtags\", StringType(), True),\n",
    "    StructField(\"tweet_id\", StringType(), True),\n",
    "    StructField(\"media\", StringType(), True),\n",
    "    StructField(\"links\", StringType(), True),\n",
    "    StructField(\"domains\", StringType(), True),\n",
    "    StructField(\"tweet_type\", StringType(), True),\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True), \n",
    "    StructField(\"a_user_id\", StringType(), True),\n",
    "    StructField(\"a_follower_count\", IntegerType(), True),\n",
    "    StructField(\"a_following_count\", IntegerType(), True),\n",
    "    StructField(\"a_is_verified\", BooleanType(), True),\n",
    "    StructField(\"a_account_creation\", IntegerType(), True),\n",
    "    StructField(\"b_user_id\", StringType(), True),\n",
    "    StructField(\"b_follower_count\", IntegerType(), True),\n",
    "    StructField(\"b_following_count\", IntegerType(), True),\n",
    "    StructField(\"b_is_verified\", BooleanType(), True),\n",
    "    StructField(\"b_account_creation\", IntegerType(), True),\n",
    "    StructField(\"a_follows_b\", BooleanType(), True),\n",
    "    StructField(\"reply\", StringType(), True),\n",
    "    StructField(\"retweet\", StringType(), True),\n",
    "    StructField(\"retweet_comment\", StringType(), True),\n",
    "    StructField(\"like\", StringType(), True)\n",
    "    ])\n",
    "# df=spark.read.csv(filenames, encoding='utf-8', sep='\\x01',header=False,schema=schema)\n",
    "# df.write.parquet(f'{base_dir}/training/pivoted_parquet/',mode=\"overwrite\")\n",
    "# 1/0\n",
    "train_df=spark.read.csv(filenames, encoding='utf-8', sep='\\x01',header=False,schema=schema).cache()\n",
    "valid_df=spark.read.csv(valid_filenames, encoding='utf-8', sep='\\x01',header=False,schema=schema).cache()\n",
    "\n",
    "# train_df=spark.read.parquet(filenames).persist(storagelevel.StorageLevel.MEMORY_ONLY).coalesce(files_nums//10*concurrentGpuTasks*gpu_amount)\n",
    "# valid_df=spark.read.parquet(valid_filenames).persist(storagelevel.StorageLevel.MEMORY_ONLY).coalesce(files_nums//10*concurrentGpuTasks*gpu_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TETransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.user_map=None\n",
    "    def _reset(self):\n",
    "        self.user_map=None\n",
    "    def fit_transform(self,train_df,valid_df,seed,cols,smooth,labels):\n",
    "        #get valid_df TE\n",
    "        target=[F.mean(label).alias(\"mean_\"+label) for label in labels]\n",
    "        te_colname='_'.join(cols)\n",
    "        te_df=train_df.groupby(cols).agg(F.count(F.lit(1)).alias(\"count\"),*target)\n",
    "        global_means=train_df.select(*target).collect()[0].asDict()\n",
    "        for k,global_mean in global_means.items():\n",
    "            label=k[4:]\n",
    "            te_df = te_df.withColumn(\"TE\"+te_colname+label,((te_df['count']*te_df['mean'+label])+(global_mean*smooth)) / (te_df['count']+smooth))\n",
    "        valid_df=valid_df.join(other=te_df.select(cols+[\"TE\"+te_colname+'_'+label for label in labels ]+[\"count\"]),on=cols,how=\"left\")\n",
    "\n",
    "        #get train K-fold TE\n",
    "        results=[]\n",
    "        kfolds=df.randomSplit([0.2 for i in range(5)],seed)\n",
    "        for i in range(5):\n",
    "            others_df=[kfolds[j] for j in range(5) if j!=i]\n",
    "            others_df = reduce(DataFrame.unionByName, others_df)\n",
    "            te_df=others_df.groupby(cols).agg(F.count(F.lit(1)).alias(\"count\"),*target)\n",
    "            global_means=others_df.select(*target).collect()[0].asDict()\n",
    "            for k,global_mean in global_means.items():\n",
    "                label=k[4:]\n",
    "                te_df = te_df.withColumn(\"TE\"+te_colname+label,((te_df['count']*te_df['mean'+label])+(global_mean*smooth)) / (te_df['count']+smooth))\n",
    "            results.append(kfolds[i].join(other=te_df.select(cols+[\"TE\"+te_colname+'_'+label for label in labels]+[\"count\"]),on=cols,how=\"left\"))\n",
    "        train_df = reduce(DataFrame.unionByName, results)\n",
    "        print(te_colname)\n",
    "        return train_df,valid_df\n",
    "class TE_arrayTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.user_map=None\n",
    "    def _reset(self):\n",
    "        self.user_map=None\n",
    "    def fit_transform(self,train_df,valid_df,seed,cols,smooth,labels):\n",
    "        #get valid_df TE\n",
    "        target=[F.mean(label).alias(\"mean_\"+label) for label in labels]\n",
    "        te_colname='_'+cols\n",
    "        te_df=train_df.withColumn(cols,F.explode(cols)).groupby(cols).agg(F.count(F.lit(1)).alias(\"count\"),*target)\n",
    "        global_means=train_df.select(*target).collect()[0].asDict()\n",
    "        for k,global_mean in global_means.items():\n",
    "            label=k[4:]\n",
    "            te_df = te_df.withColumn(\"TE\"+te_colname+label,((te_df['count']*te_df['mean'+label])+(global_mean*smooth)) / (te_df['count']+smooth))\n",
    "        mean_valid_df=valid_df.withColumn(cols,F.explode(cols)).join(other=te_df.select([cols]+[\"TE\"+te_colname+'_'+label for label in labels ]+[\"count\"]),on=cols,how=\"left\").\\\n",
    "        groupby([\"a_user_id\",\"b_user_id\",\"timestamp\",'tweet_id']).agg(F.count(F.lit(1)).alias(\"count\"),*[F.mean(\"TE\"+te_colname+'_'+label).alias(\"TE\"+te_colname+'_'+label) for label in labels ])\n",
    "        print(\"before array:\",valid_df.count(),mean_valid_df.count())\n",
    "        valid_df=valid_df.join(other=mean_valid_df.select([\"a_user_id\",\"b_user_id\",\"timestamp\",'tweet_id']+[\"TE\"+te_colname+'_'+label for label in labels ]+[\"count\"]),on=[\"a_user_id\",\"b_user_id\",\"timestamp\",'tweet_id'],how=\"left\")\n",
    "        print(\"after array:\",valid_df.count())\n",
    "        #get train K-fold TE\n",
    "        results=[]\n",
    "        kfolds=train_df.randomSplit([0.2 for i in range(5)],seed)\n",
    "        for i in range(5):\n",
    "            others_df=[kfolds[j] for j in range(5) if j!=i]\n",
    "            others_df = reduce(DataFrame.unionByName, others_df)\n",
    "            te_df=others_df.withColumn(cols,F.explode(cols)).groupby(cols).agg(F.count(F.lit(1)).alias(\"count\"),*target)\n",
    "            global_means=others_df.select(*target).collect()[0].asDict()\n",
    "            for k,global_mean in global_means.items():\n",
    "                label=k[4:]\n",
    "                te_df = te_df.withColumn(\"TE\"+te_colname+label,((te_df['count']*te_df['mean'+label])+(global_mean*smooth)) / (te_df['count']+smooth))\n",
    "            mean_kfolds=kfolds[i].withColumn(cols,F.explode(cols)).join(other=te_df.select([cols]+[\"TE\"+te_colname+'_'+label for label in labels ]+[\"count\"]),on=cols,how=\"left\").\\\n",
    "            groupby([\"a_user_id\",\"b_user_id\",\"timestamp\",'tweet_id']).agg(F.count(F.lit(1)).alias(\"count\"),*[F.mean(\"TE\"+te_colname+'_'+label).alias(\"TE\"+te_colname+'_'+label) for label in labels ])\n",
    "            print(\"before array:\",kfolds[i].count())\n",
    "            result=kfolds[i].join(other=mean_kfolds.select([\"a_user_id\",\"b_user_id\",\"timestamp\",'tweet_id']+[\"TE\"+te_colname+'_'+label for label in labels ]+[\"count\"]),on=[\"a_user_id\",\"b_user_id\",\"timestamp\",'tweet_id'],how=\"left\")\n",
    "            print(\"after array:\",result.count())\n",
    "            results.append(result)\n",
    "        train_df = reduce(DataFrame.unionByName, results)\n",
    "        print(te_colname)\n",
    "        return train_df,valid_df\n",
    "seed=42\n",
    "cols=[\"tweet_type\",\"language\"]\n",
    "smooth=20\n",
    "labels=['reply', 'retweet', 'retweet_comment', 'like']\n",
    "# for c in [\n",
    "#     ['b_user_id','tweet_type','language'],\n",
    "#     ['a_user_id'],\n",
    "#     ['b_user_id'],\n",
    "#     ['a_user_id'],\n",
    "#     ['b_user_id','b_user_id'],\n",
    "\n",
    "# ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_reply': 0.029620271188720005, 'mean_retweet': 0.08751765877737647, 'mean_retweet_comment': 0.007051974651785061, 'mean_like': 0.398031314129461}\n",
      "16150\n"
     ]
    }
   ],
   "source": [
    "preprocessTransformer=PreprocessTransformer()\n",
    "train_df=preprocessTransformer.fit_transform(train_df)\n",
    "valid_df=preprocessTransformer.transform(valid_df,update=True)\n",
    "userTransformer=UserTransformer()\n",
    "userTransformer.fit(train_df)\n",
    "train_df=userTransformer.transform(train_df).cache()\n",
    "valid_df=userTransformer.transform(valid_df).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_col = [feat for feat in train_df.schema.names if feat not in target_col ]\n",
    "params = {'eta': 0.1, 'gamma': 0.1, 'missing': 0.0,\n",
    "          'treeMethod': 'gpu_hist', 'maxDepth': 3, \n",
    "          'growPolicy': 'depthwise', 'lambda_': 1.0,\n",
    "          'subsample': 1.0, 'numRound': 1000,\n",
    "          'numWorkers': 1, 'verbosity': 1}\n",
    "tE_arrayTransformer=TE_arrayTransformer()\n",
    "seed=42\n",
    "cols=[\"tweet_type\",\"language\"]\n",
    "smooth=20\n",
    "labels=['reply', 'retweet', 'retweet_comment', 'like']\n",
    "array_cols=[\"hashtags\",\"links\",\"domains\"]\n",
    "for array_col in array_cols:\n",
    "    train_df,valid_df=tE_arrayTransformer.fit_transform(train_df,valid_df,seed,array_col,smooth,labels)\n",
    "    train_df=train_df.cache()\n",
    "    valid_df=valid_df.cache()\n",
    "xgboost = XGBoostClassifier(**params).setLabelCol(target_col).setFeaturesCols(features_col)\n",
    "model = xgboost.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
